# -*- coding: utf-8 -*-
"""question2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iyGEkCWe6wl2QAGjBuboMuPI5wi0dsjC
"""

################## Execute in Google Colab after loading the images in your drive ##################

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import os
import cv2
import skimage
import matplotlib.pyplot as plt
import scipy.io as sio
!cp 'drive/My Drive/Project_1/mywarper.py' mywarper.py
import mywarper
import matplotlib.colors as colors
from sklearn.preprocessing import StandardScaler
from skimage import color
import tensorflow as tf
from copy import deepcopy

def ld_images_drive(folder):
  images = []
  for file in sorted(os.listdir('drive/My Drive/Project_1/images')):
    img = cv2.imread(os.path.join(folder,file))
    if img is not None:
      images.append(img)
  return np.asarray(images)

def ld_landmarks_drive(folder):
    matrices=[]
    for file in sorted(os.listdir('drive/My Drive/Project_1/landmarks')):
        matrix = sio.loadmat(os.path.join(folder,file))['lms']
        if matrix is not None:
            matrices.append(matrix)
    return np.asarray(matrices)

def select_device(use_gpu=True):
    from tensorflow.python.client import device_lib
    device = '/device:GPU:0' if use_gpu else '/CPU:0'
    return device

USE_GPU = True
device = select_device(use_gpu=USE_GPU)
  
all_images_rgb = ld_images_drive("drive/My Drive/Project_1/images")
all_images = []
for i in range(len(all_images_rgb)):
    all_images.append(cv2.cvtColor(all_images_rgb[i], cv2.COLOR_BGR2RGB))
all_landmarks = ld_landmarks_drive("drive/My Drive/Project_1/landmarks")
all_landmarks = all_landmarks.reshape(1000,136)
train_landmarks = all_landmarks[:800,]
mean_landmarks = np.mean(train_landmarks, axis=0)

warp_all_images = []
for i in range(len(all_images)):
    warp_all_images.append(mywarper.warp(all_images[i].reshape(128,128,3),all_landmarks[i].reshape(68,2),mean_landmarks.reshape(68,2)))
warp_all_images = np.array(warp_all_images)
warp_all_images = warp_all_images / 255
train_images = warp_all_images[:800,]
test_images = warp_all_images[800:,]
all_landmarks = all_landmarks / 128
train_landmarks = all_landmarks[:800,]
test_landmarks = all_landmarks[800:,]

learning_rate = 7e-4
num_steps = 300
batch_size = 100
disp_step = 10

tf.reset_default_graph()

app = tf.placeholder("float32", (None, 128, 128, 3))
lm = tf.placeholder("float32", (None, 136))
app_target = tf.placeholder("float32", (None, 128, 128, 3))
lm_target = tf.placeholder("float32", (None, 136))

conv1_1 = tf.layers.conv2d(app, strides = 2, filters = 16, kernel_size = 5, padding = 'SAME')
conv1_2 = tf.nn.leaky_relu(conv1_1)
conv2_1 = tf.layers.conv2d(conv1_2, strides = 2, filters = 32, kernel_size = 3, padding = 'SAME')
conv2_2 = tf.nn.leaky_relu(conv2_1)
conv3_1 = tf.layers.conv2d(conv2_2, strides = 2, filters = 64, kernel_size = 3, padding = 'SAME')
conv3_2 = tf.nn.leaky_relu(conv3_1)
conv4_1 = tf.layers.conv2d(conv3_2, strides = 2, filters = 128, kernel_size = 3, padding = 'SAME')
conv4_2 = tf.nn.leaky_relu(conv4_1)
conv5_1 = tf.layers.conv2d(conv4_2, strides = 1, filters = 50, kernel_size = 8, padding = 'VALID')
conv5_2 = tf.nn.leaky_relu(conv5_1)

deconv1_1 = tf.layers.conv2d_transpose(conv5_2, strides = 1, filters = 128, kernel_size = 8, padding = 'VALID')
deconv1_2 = tf.nn.leaky_relu(deconv1_1)
deconv2_1 = tf.layers.conv2d_transpose(deconv1_2, strides = 2, filters = 64, kernel_size = 3, padding = 'SAME')
deconv2_2 = tf.nn.leaky_relu(deconv2_1)
deconv3_1 = tf.layers.conv2d_transpose(deconv2_2, strides = 2, filters = 32, kernel_size = 3, padding = 'SAME')
deconv3_2 = tf.nn.leaky_relu(deconv3_1)
deconv4_1 = tf.layers.conv2d_transpose(deconv3_2, strides = 2, filters = 16, kernel_size = 3, padding = 'SAME')
deconv4_2 = tf.nn.leaky_relu(deconv4_1)
deconv5_1 = tf.layers.conv2d_transpose(deconv4_2, strides = 2, filters = 3, kernel_size = 5, padding = 'SAME')
deconv5_2 = tf.nn.sigmoid(deconv5_1)

encoded_app, pred_app = conv5_2, deconv5_2

loss_app = tf.losses.mean_squared_error(app_target, pred_app)
optimizer_app = tf.train.AdamOptimizer(learning_rate).minimize(loss_app)

init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
for i in range(num_steps):
  for j in range(0, 800, batch_size):
    feed_dict = {app: train_images[j:j+100,], app_target: train_images[j:j+100,]}
    _, l = sess.run([optimizer_app, loss_app], feed_dict = feed_dict)
    if i % disp_step == 0:
      print("Step ",i,": Minibatch Loss: ",l)
result_app = []
for i in range(0,200,batch_size):
  feed_dict = {app: test_images[i:i+100,], app_target: test_images[i:i+100,]}
  tmp = sess.run(pred_app, feed_dict = feed_dict)
  result_app.append(tmp)
result_app = np.array(result_app)
result_app = result_app.reshape(200,128,128,3)
ae_app = []
for i in range(0,800,batch_size):
  feed_dict = {app: train_images[i:i+100,], app_target: train_images[i:i+100,]}
  tmp = sess.run(encoded_app, feed_dict = feed_dict)
  ae_app.append(tmp)
ae_app = np.array(ae_app)
ae_app = ae_app.reshape(800,50)

var = np.var(ae_app, axis=0)
new_app = []
for i in range(4):
  tmp = np.argmax(var)
  var[tmp] = 0
  mn = min(ae_app[:,tmp])
  mx = max(ae_app[:,tmp])
  for j in range(10):
    img = deepcopy(ae_app[5])
    img[tmp] = mn + j*((mx-mn)/10)
    new_app.append(img)
new_app = np.array(new_app)

feed_dict = {conv5_2: new_app.reshape(40,1,1,50)}
new_images_app = sess.run(pred_app, feed_dict = feed_dict)

for i in range(4):
  fig_plot_index = 0
  fig = plt.figure(figsize=(20,5))
  for j in range(10):
      fig_plot_index += 1
      fig.add_subplot(1, 10, fig_plot_index)
      plt.imshow(new_images_app[(i*10)+j])
  plt.show()

lm_conv1_1 = tf.contrib.layers.fully_connected(lm, num_outputs=100,activation_fn=None)
lm_conv1_2 = tf.nn.leaky_relu(lm_conv1_1)
lm_conv2_1 = tf.contrib.layers.fully_connected(lm_conv1_2, num_outputs=10,activation_fn=None)
lm_conv2_2 = tf.nn.leaky_relu(lm_conv2_1)

lm_deconv1_1 = tf.contrib.layers.fully_connected(lm_conv2_2, num_outputs=100,activation_fn=None)
lm_deconv1_2 = tf.nn.leaky_relu(lm_deconv1_1)
lm_deconv2_1 = tf.contrib.layers.fully_connected(lm_deconv1_2, num_outputs=136,activation_fn=None)
lm_deconv2_2 = tf.nn.sigmoid(lm_deconv2_1)

encoded_lm, pred_lm = lm_conv2_2, lm_deconv2_2

loss_lm = tf.losses.mean_squared_error(lm_target, pred_lm)
optimizer_lm = tf.train.AdamOptimizer(learning_rate).minimize(loss_lm)

init=tf.global_variables_initializer()
sess.run(init)
for i in range(num_steps):
  for j in range(0, 800, batch_size):
    feed_dict = {lm: train_landmarks[j:j+100,], lm_target: train_landmarks[j:j+100,]}
    _, l = sess.run([optimizer_lm, loss_lm], feed_dict = feed_dict)
    if i % disp_step == 0:
      print("Step ",i,": Minibatch Loss: ",l)
result_lm = []
for i in range(0,200,batch_size):
  feed_dict = {lm: test_landmarks[i:i+100,], lm_target: test_landmarks[i:i+100,]}
  tmp = sess.run(pred_lm, feed_dict = feed_dict)
  result_lm.append(tmp)
result_lm = np.array(result_lm)
result_lm = result_lm.reshape(200,136)
ae_lm = []
for i in range(0,800,batch_size):
  feed_dict = {lm: train_landmarks[i:i+100,], lm_target: train_landmarks[i:i+100,]}
  tmp = sess.run(encoded_lm, feed_dict = feed_dict)
  ae_lm.append(tmp)
ae_lm = np.array(ae_lm)
ae_lm = ae_lm.reshape(800,10)

result_lm = result_lm * 128
warp_final_images = []
for i in range(len(result_app)):
    warp_final_images.append(mywarper.warp(result_app[i].reshape(128,128,3),mean_landmarks.reshape(68,2),result_lm[i].reshape(68,2)))
warp_final_images = np.array(warp_final_images)

fig_plot_index = 0
fig = plt.figure(figsize=(10,40))
for i in range(20):
    fig_plot_index += 1
    fig.add_subplot(20, 2, fig_plot_index)
    plt.imshow(all_images[800+i])
    fig_plot_index += 1
    fig.add_subplot(20, 2, fig_plot_index)
    plt.imshow(warp_final_images[i])
plt.show()

var = np.var(ae_lm, axis=0)
new_lm = []
for i in range(2):
  tmp = np.argmax(var)
  var[tmp] = 0
  mn = min(ae_lm[:,tmp])
  mx = max(ae_lm[:,tmp])
  for j in range(10):
    img = deepcopy(ae_lm[5])
    img[tmp] = mn + j*((mx-mn)/10)
    new_lm.append(img)
new_lm = np.array(new_lm)

feed_dict = {lm_conv2_2: new_lm.reshape(20,10)}
new_images_lm = sess.run(pred_lm, feed_dict = feed_dict)

warp_interpolated_images = []
for i in range(len(new_images_lm)):
    warp_interpolated_images.append(mywarper.warp(result_app[5].reshape(128,128,3),mean_landmarks.reshape(68,2),new_images_lm[i].reshape(68,2)*128))
warp_interpolated_images = np.array(warp_interpolated_images)

for i in range(2):
  fig_plot_index = 0
  fig = plt.figure(figsize=(20,5))
  for j in range(10):
      fig_plot_index += 1
      fig.add_subplot(1, 10, fig_plot_index)
      plt.imshow(warp_interpolated_images[(i*10)+j])
  plt.show()